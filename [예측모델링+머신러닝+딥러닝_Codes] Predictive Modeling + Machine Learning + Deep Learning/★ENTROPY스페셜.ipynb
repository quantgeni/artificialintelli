{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"★ENTROPY스페셜.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"D9jc1o2afYOH","colab_type":"code","colab":{}},"source":["# 엔트로피entropy\n","# 물리 열역학의 관점에서의 정의\n","# => 물질의 열적 상태를 나타내는 물리량의 하나\n","# => 보통 무질서도 라고 함\n","# => 엔트로피가 높으면 무질서도 증가\n","\n","# 통계적 관점에서의 정의\n","# => 정보파악의 혼잡도\n","# => 엔트로피가 높으면 정보이해가 어려워짐\n","\n","# 학습데이터는 기본적으로 혼잡한 상태임. 따라서 어떤 조건으로 분류해야만 전체 혼잡도가 개선되는지 계산하고\n","# 그 조건을 찾아내서 실제로 정리함. 정리한 결과에 대해 다시 계산해서 처리를 반복 적용함\n","\n","# 정보이론information theory에서 정보의 불확실성을 수치로 나타낸 것을 엔트로피entropy라 함\n","\n","# ex) x를 기준으로 y를 나누는 가장 좋은 방법은?\n","x = [1,2,3,4,5,6,7,8]\n","y = [0,0,0,1,1,1,1,1]\n","\n","# 방법1) 분류기준 : x < 3.5 <= 최적의 조건\n","# 방법2) 분류기준 : x < 4.5 <= 오분류"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"euxVIHjoiozu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"416976a1-7519-453d-f237-601b5b00b92f","executionInfo":{"status":"ok","timestamp":1562573410008,"user_tz":-540,"elapsed":665,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}}},"source":["# 엔트로피의 식\n","# 복수의 현상 (1~n)이 존재할 때 그 혼잡도를 나타내는 엔트로피는 \n","\n","# -p(현상1) * log(p(현상1)) +\n","# -p(현상2) * log(p(현상2) +\n","# ... ...\n","# -p(현상n) * log(p(현상n)) + 이다 -_-;\n","\n","# ex) 확률 0.1의 현상이 10번 일어난 경우 엔트로피는?\n","# (0.1 * log2(0.1)) * 10 * -1 + ...\n","\n","import math\n","\n","(0.1 * math.log2(0.1)) * 10 * -1 \n","# ..."],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.3219280948873626"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"5s19frK0ogvA","colab_type":"code","colab":{}},"source":["## 엔트로피 entropy\n","\n","# 물리 열역학의 관점에서의 정의\n","# => 물질의 열적 상태를 나타내는 물리량의 하나\n","# => 보통 무질서도 라고 함\n","# => 엔트로피가 높으면 무질서도 증가\n","\n","# 통계적 관점에서의 정의\n","# => 정보이득(information gain)의 혼잡도\n","# => 엔트로피가 높으면 정보이해가 어려워짐\n","\n","# 학습데이터는 기본적으로 혼잡한 상태임\n","# 따라서, 어떤 조건으로 분류해야만 전체 혼잡도가\n","# 개선되는지 계산하고 그 조건을 찾아내서 실제로 정리함\n","# 정리한 결과에 대해 다시 계산해서 처리를 반복 적용함\n","\n","# 정보이론information theory에서 정보의 불확실성을\n","# 수치로 나타낸 것을 엔트로피entropy라 함\n","\n","\n","# ex) x를 기준으로 y를 나누는 가장 좋은 방법은?\n","# x = [1, 2, 3, 4, 5, 6, 7, 8]\n","# y = [0, 0, 0, 1, 1, 1, 1, 1]\n","\n","\n","# 방법 1) 분류 기준 : x < 3.5  <= 최적의 조건\n","# 방법 2) 분류 기준 : x < 4.5  <= 오분류 조건\n","\n","\n","# 엔트로피의 식\n","# 복수의 현상(1~n)이 존재할 때 그 혼잡도를 나타내는 엔트로피 식은 \n","# -p(현상1) * log(p(현상1)) + \n","# -p(현상2) * log(p(현상2)) +\n","# ... ...\n","# -p(현상n) * log(p(현상n)) 이다\n","\n","# 참고 - 엔트로피는 일반적으로 자연로그를 이용해서 계산함!!\n","# 단!! 아래 예제에서는 2를 밑으로 하는 로그로 엔트로피를 계산했음\n","\n","\n","\n","# ex) 확률 0.1의 현상이 10개 일어난 경우 엔트로피는?\n","# (0.1 * log2(0.1)) * 10 * -1 = 3.3219280948873626\n","\n","import math\n","(0.1 * math.log2(0.1)) * 10 * -1\n","\n","\n","# ex) 확률 0.25의 현상이 4개 일어난 경우 엔트로피는?\n","(0.25 * math.log2(0.25)) * 4 * -1\n","\n","\n","\n","# ex) 확률 0.5의 현상이 2개 일어난 경우 엔트로피는?\n","(0.5 * math.log2(0.5)) * 2 * -1\n","\n","\n","\n","# ex) 10마리의 동물이 개인지 고양이인지 분류\n","# big   follow   walking   target\n","# yes   yes      yes       dog\n","# yes   yes      no        cat\n","# no    yes      yes       dog\n","# yes   yes      yes       dog\n","# no    no       yes       cat\n","\n","# yes   no       yes       dog\n","# yes   no       yes       cat\n","# no    no       no        cat\n","# yes   yes      yes       dog\n","# no    yes      no        dog\n","\n","\n","# 1) target 엔트로피 계산\n","# 10마리 중 6마리는 개, 4마리는 고양이로 분류\n","# dog : cat = 6 : 4\n","# 개로 분류될 확률       : 6/10 = 0.6\n","# 고양이로 분류될 확률   : 4/10 = 0.4\n","\n","# 따라서, 엔트로피\n","-(0.6 * log2(0.6)) -(0.4 * log2(0.4))\n","# => -(0.6 * math.log2(0.6)) -(0.4 * math.log2(0.4))\n","# => -0.44217935649972373 -0.5287712379549449\n","# => 0.9709505944546686 = 0.971\n","\n","\n","\n","# 2) 덩치기준으로 분류시 엔트로피 계산\n","\n","# 덩치가 큰 것으로 분류  \n","# 개 : 고양이 = 4 : 2\n","# 덩치가 큰 경우 개로 분류될 확률 : 4/6 = 0.6666666666666666\n","# 덩치가 큰 경우 고양이로 분류될 확률 : 2/6 = 0.3333333333333333\n","# 따라서, -(0.667 * log2(0.667)) -(0.333 * log2(0.333))\n","# => -(0.667 * math.log2(0.667)) -(0.333 * math.log2(0.333))\n","# => 0.9179621399872384 = 0.918\n","\n","\n","# 덩치가 작은것으로 분류\n","# 개 : 고양이 = 2 : 2\n","# 덩치가 작은 경우 개/고양이로 분류될 확률 2/4 = 0.5\n","# => -(0.5 * math.log2(0.5)) -(0.5 * math.log2(0.5))\n","# => 0.5 + 0.5 = 1.0\n","\n","\n","# 덩치로 분류했을때의 엔트로피는\n","# 개로 분류될 확률 x 덩치로 분류될 확률 과 \n","# 고양이로 분류될 확률 x 덩치로 분류될 확률을\n","# 더해준 결과값이 됨\n","0.6 * 0.918 + 0.4 * 1 = 0.9508\n","\n","\n","\n","# 3) 따름기준으로 분류시 엔트로피 계산\n","\n","# 잘 따름으로 분류  \n","# 개 : 고양이 = 5:1\n","\n","# 잘따르는데 개로 분류될 확률\n","# 5/6 = 0.8333333333333334\n","\n","# 잘 따르는데 고양이로 분류될 확률\n","# 1/6 = 0.16666666666666666\n","# 따라서, \n","# => -(0.833 * math.log2(0.833)) -(0.167 * math.log2(0.167))\n","# => 0.21958846221401737 -0.43120735869540183\n","0.220 + 0.431 = 0.651 \n","\n","\n","\n","# 잘 안따름으로 분류\n","# 개 : 고양이 = 1:3\n","\n","# 잘 안따르는데 개로 분류될 확률 \n","# 1/4 = 0.25 \n","\n","# 잘 안따르는데 고양이로 분류될 확률\n","# 3/4 = 0.75\n","# 따라서, \n","# => -(0.25 * math.log2(0.25)) -(0.75 * math.log2(0.75))\n","# => 0.5- 0.31127812445913283 = -0.811\n","\n","\n","\n","# 덩치로 분류했을때의 엔트로피는\n","0.6 * 0.651 + 0.4 * 0.811 = 0.715\n","\n","\n","\n","\n","\n","\n","# 4) 산책기준으로 분류시 엔트로피 계산\n","\n"],"execution_count":0,"outputs":[]}]}