{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SK예측모델링1(TEXT>레이블링).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"UZ_N-xwBG45K","colab_type":"code","colab":{}},"source":["# 일반적으로 머신러닝은 데이터가공/변환 (전처리)  -> 모델학습/예측 -> 평가 의 과정을 거침\n","\n","# 앞의 타이타닉 예제에서 모델의 평가는 정확도만 사용했음. 한편, 머신러닝의 예측성능의 평가방법은 다양함\n","\n","# 회귀 - R^2, MSE평균제곱오차\n","# 분류 - 혼동행렬, 크로스엔트로피, 최대우도, ROU, ARC, F1 스코어\n","\n","# 정확도accuracy\n","# 맞는 것을 맞다고 틀린 것을 틀리다고 올바르게 예측하는 것\n","# TP + TN / (TP + FN + FP + TN)\n","# 혼동행렬 대각선 부분\n","\n","\n","# 정밀도precision\n","# 모델의 예측값이 얼마나 정확하게 맞게 예측되었는지 알아봄\n","# TP / (TP + FP)\n","# 혼동행렬 1열 부분\n","\n","# 민감도sensitivity\n","# 맞는 것 중 맞다고 예측된 것들의 비율 = 재현율\n","\n","# 특이도specificity\n","# 틀린 것 중 틀리다고 예측된 것들의 비율\n","# (FP) / (TN + FP)\n","# 혼동 행렬의 2행 부분\n","\n","# 정확도 지표만으로는 ML 모델의 성능을 파악하기에는 다소 문제가 있음 - 왜곡의 위험\n","\n","# 즉, 탐색적 분석을 시행했을 때 성별을 기준으로 생존비율은 여성일 때가 더 높았음.\n","# 따라서 굳이 ML 알고리즘을 적용하지 않아도 성별이 여성일 경우 생존, 남성일 경우 사망이라고 예측해도 크게 무리 없음\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWiL14F4Hnl5","colab_type":"code","colab":{}},"source":["# 다양한 머신러닝 알고리즘을 이용.\n","# 교차검증 방식으로 모델을 훈련시키고 예측 정확도를 평가해 줌\n","\n","import sklearn\n","import image\n","import seaborn as sns\n","from sklearn.datasets import load_iris\n","from sklearn.datasets import load_boston\n","from sklearn.datasets import load_digits\n","from sklearn.datasets import fetch_lfw_people\n","from sklearn.datasets import make_blobs\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","import numpy as np\n","import pandas as pd\n","from pandas.plotting import scatter_matrix\n","import matplotlib.pyplot as plt\n","from sklearn.base import BaseEstimator\n","from sklearn.datasets import load_digits\n","\n","# predictive modeling 계열 (의사결정나무, 랜덤포레스트, 로지스틱 회귀, Knearest 등) <- Kmeans 어따 빼먹음\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# 모델 검증 관련 (정확도)\n","from sklearn.metrics import accuracy_score\n","\n","# preprocessing (전처리) 그룹\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import RobustScaler \n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","\n","# pandas 출력 설정 : 출력시 가로 생략 없애기\n","# pd.set_option('display.max_columns', 50)\n","# pd.set_option('display.width',250)\n","pd.set_option('display.expand_frame_repr', False) \n","# 바로 위 1줄이면 그 위위 2줄과 동일\n","\n","from sklearn.base import BaseEstimator\n","\n","from sklearn.metrics import confusion_matrix # 혼동행렬\n","\n","# score류 (predictive modeling을 평가하는 지표)\n","from sklearn.metrics import accuracy_score # 정확도\n","from sklearn.metrics import precision_score # 정밀도\n","from sklearn.metrics import recall_score # 민감도(재현율)\n","from  sklearn.metrics import f1_score # F1 스코어 (정밀도, 민감도 조화평균..)\n","from sklearn.metrics import roc_auc_score\n","\n","# ROC\n","\n","# Receiver Operation Characteristics Curve\n","# 수신자 판단 곡선\n","\n","# 세계 2차 대전 통신 장비 성능 평가를 위해 고안된 수치\n","# 의학분야에 많이 사용되지만, 머신러닝의 이진 분류 모델 예측 성능 평가에도 사용\n","\n","# 특이도(FPR)가 변할 때 민감도가 어떻게 변하는지 알아보기 위한 곡선\n","\n","# 환자 중 보균자p/정상인n 있는 경우\n","# 재현율 : 보균자를 보균자로 양성 판정\n","# 특이도 : 정상인을 정상인으로 음성 판정\n","\n","# -_-;;; -------------------------------------------------------------------------------\n","\n","\n","# AUC \n","\n","# Area Under Curve\n","# ROC 곡선 밑의 면적을 구한 값\n","# 1에 가까울수록 좋은 수치를 의미함"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7m54aOeL1XU","colab_type":"code","outputId":"13a840b4-046c-47b9-e243-eb7bc89929e5","executionInfo":{"status":"ok","timestamp":1561958312500,"user_tz":-540,"elapsed":888,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","# BaseEstimator 클래스를 상속받아\n","# 성별이 남성이면 사망, 여성이면 생존이라고 예측하는데 \n","# 더미분류기를 하나 생성\n","\n","from sklearn.base import BaseEstimator\n","\n","class MyDummyClassifier(BaseEstimator):\n","  # 아무것도 학습하지 않는 fit 메서드 정의\n","  def fit(self, X, y=None):\n","    pass\n","  \n","  # 성별이 1(남성)이면 0(사망), 0(여성)이면 1(생존)\n","  def predict(self, X):\n","    pred = np.zeros((X.shape[0], 1))\n","    # 입력데이터 크기만큼 0으로 채워진 1차원 행렬 생성\n","    \n","    for i in range(X.shape[0]):\n","      if X['Sex'].iloc[i] != 1:\n","        pred[i] = 1\n","      # 성별이 여성인 경우 1로 설정  \n","        \n","    return pred\n","\n","\n","# 이렇게 만들어진 분류기를 타이타닉 데이터에 적용\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","\n","# 원본데이터 로딩 및 전처리\n","titanic = pd.read_csv('titanic.csv')\n","titanic_target = titanic['Survived']\n","titanic_data = titanic.drop('Survived', axis=1)\n","\n","\n","# 데이터 전처리\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","# null 처리\n","titanic['Age'].fillna(titanic['Age'].mean(), inplace=True)\n","titanic['Cabin'].fillna('N', inplace=True)\n","titanic['Embarked'].fillna('N', inplace=True)\n","titanic['Fare'].fillna(0, inplace=True)\n","\n","\n","# 불필요한 속성 제거\n","titanic.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n","\n","\n","# 레이블 인코딩 적용\n","titanic['Cabin'] = titanic['Cabin'].str[:1]\n","# 선실 위치의 첫글자를 추출(deck 단위)\n","\n","#print(titanic['Cabin'])\n","\n","features = ['Cabin', 'Sex', 'Embarked']\n","for feature in features:\n","  #print(feature)\n","  le = LabelEncoder()\n","  le = le.fit(titanic[feature])\n","  titanic[feature] = le.transform(titanic[feature])\n","  #print(titanic[feature])\n","\n","titanic_target = titanic['Survived']\n","titanic_data = titanic.drop('Survived', axis=1)\n","\n","\n","X_train, X_test, y_train, y_test = \\\n","train_test_split(titanic_data, titanic_target, test_size=0.2, random_state=0)\n","\n","\n","\n","# 가짜 분류기에 데이터 적용\n","my_clf = MyDummyClassifier()\n","\n","my_clf.fit(X_train, y_train)\n","\n","mypred = my_clf.predict(X_test)\n","print('정확도', accuracy_score(y_test, mypred))\n","# 0.7877\n","\n","# 즉, 이렇게 단순한 알고리즘만으로 예측하더라도 \n","# 데이터의 구성에 따라 정확도가 약 78.77%가 나옴\n","# 불균형한 레이블의 비율에서는 정확도 지표로 \n","# 모델 성능을 평가하는 것은 올바르지 않음\n","titanic_data['Sex'].value_counts()\n","titanic_target.value_counts()\n","\n","\n","# 예를 들어 100개의 종속변수 중 90개가 0이고\n","# 10개가 1인 경우 무조건 0으로 예측결과를 반환하는 \n","# 머신러닝 알고리즘의 정확도는 90% 임!!"],"execution_count":0,"outputs":[{"output_type":"stream","text":["정확도 0.7877094972067039\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0    549\n","1    342\n","Name: Survived, dtype: int64"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"Oi0e3931N3ZM","colab_type":"code","outputId":"f53c48f4-a80f-484e-fc03-3d20c1a32f2b","executionInfo":{"status":"ok","timestamp":1561958719885,"user_tz":-540,"elapsed":841,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# MINST 데이터셋을 변환해서 불균형한 데이터셋으로 만든 후,\n","# 정확도 지표사용시 어떤 문제가 발생하는지 알아봄\n","\n","from sklearn.base import BaseEstimator\n","\n","class MyFakeClassifier(BaseEstimator):\n","  def fit(self, X, y_None):\n","    pass\n","  \n","  def predict(self, X):\n","    return np.zeros((len(X), 1), dtype=bool)\n","    # 입력값 X에 대해 데이터 크기만큼 \n","    # 0으로 채운 행렬을 만들어 결과로 리턴 \n","    # 즉, 입력값에 상관없이 무조건 0을 반환\n","\n","    # pred = np.zeros((X.shape[0], 1))\n","    # 입력데이터 크기만큼 0으로 채워진 1차원 행렬 생성\n","\n","from sklearn.datasets import load_digits\n","\n","digits = load_digits()\n","#print(digits)\n","#print('-------------')\n","\n","y = (digits.target == 7).astype(int)\n","# digit 이 7인 데이터만 골라서 1로 변환해서 target(종속변수)으로 지정\n","\n","#print(y)\n","#print('-------------')\n","\n","X_train, X_test, y_train, y_test = \\\n","train_test_split(digits.data, y, random_state=11)\n","\n","\n","# 데이터 분포 확인\n","print('레이블(종속변수) 크기', y_test.shape)\n","print('-------------')\n","print('레이블 분포', pd.Series(y_test).value_counts())\n","print('-------------')\n","# 0 405\n","# 1 45\n","\n","print(y_test)\n","print(pd.Series(y_test))\n","\n","my_clf = MyFakeClassifier()\n","my_clf.fit(X_train, y_train)\n","mypred = my_clf.predict(X_test)\n","print('정확도', accuracy_score(y_test, mypred))\n","\n","\n","# 0.900\n","# 예측을 0으로만 했는데도 정확도 90% 달성!!\n","# => 실제 예측범위는 0 ~ 9 사이!!\n","\n","# 예를 들어, 선생님이 시험 정답지의 답을 1로만 작성하는 경우,\n","# 학생이 1로 찍기만 해도 100점을 맞는 경우와 비슷한 상황임!"],"execution_count":0,"outputs":[{"output_type":"stream","text":["레이블(종속변수) 크기 (450,)\n","-------------\n","레이블 분포 0    405\n","1     45\n","dtype: int64\n","-------------\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0\n"," 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n"," 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0\n"," 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n"," 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0]\n","0      0\n","1      0\n","2      0\n","3      0\n","4      0\n","5      0\n","6      0\n","7      0\n","8      0\n","9      0\n","10     0\n","11     0\n","12     0\n","13     0\n","14     0\n","15     0\n","16     0\n","17     0\n","18     0\n","19     0\n","20     0\n","21     0\n","22     0\n","23     0\n","24     0\n","25     0\n","26     0\n","27     0\n","28     0\n","29     0\n","      ..\n","420    0\n","421    1\n","422    0\n","423    0\n","424    0\n","425    0\n","426    0\n","427    0\n","428    0\n","429    1\n","430    1\n","431    1\n","432    0\n","433    0\n","434    0\n","435    0\n","436    0\n","437    0\n","438    0\n","439    0\n","440    0\n","441    0\n","442    0\n","443    0\n","444    0\n","445    0\n","446    0\n","447    0\n","448    0\n","449    0\n","Length: 450, dtype: int64\n","정확도 0.9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qOZeYlGkg2rd","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"sGv1Vhb3SOpz","colab_type":"code","colab":{}},"source":["# 따라서 정확도 평가 지표는 불균형한 레이블 데이터셋에서의 성능수치로 사용되면 안됨\n","# => 이러한 한계를 극복하기 위해 혼동행렬 사용\n","# => 특히 정확도보다는 정밀도, 정밀도 보다는 민감도"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0c9x4u4ChHto","colab_type":"code","colab":{}},"source":["# from sklearn.metrics import confusion_matrix\n","\n","# confusion_matrix(y_test, mypred)\n","\n","#                   negative   positive(prediction)\n","# (actual) negative  [405,        0]\n","#         positive  [45,         0]\n","  \n","  \n","# 정확도\n","# (TN + TP) / (TN + TP + FN + FP)\n","# = (405 + 0) / (405 + 0 + 45 + 0) = 0.9\n","\n","# 정밀도 (positive 예측여부 : 양성 예측도)\n","\n","# 재현율(정답여부 : 민감도) -_-;; 모르겠고\n","\n","\n","# Thus in binary classification,\n","# the count of true negatives is C0, 0,\n","# false negatives is C1, 0,\n","# true positives is C1, 1,\n","# and false positives is C0,1..\n","\n","\n","# ex) 정확도 : 나의 예측이 얼마나 정확한가?\n","# ex) 정밀도 : 나의 참이라고 예측한 것이 얼마나 정확한가?\n","# ex) 재현율 : 전체 참 중에서 내가 참이라 예측한 것은?\n","\n","\n","# ex) 6마리의 동물 중 개p/고양이n를 맞추는 게임을\n","# 정답 = [개 개 개 고 개 고]\n","# 예측 = [개 고 개 개 고 개]\n","\n","# ex) 정확도 : 나의 예측이 얼마나 정확한가? 2/6 => 33.3%\n","# ex) 정밀도 : 나의 참이라고 예측한 것이 얼마나 맞았나? 2/4 => 50%\n","# ex) 재현율 : 전체 참 중에서 내가 참이라 예측한 것은? 2/4 => 50%\n","\n","\n","\n","\n","# ex) 노래하는 모습을 보고 음치를 찾아봄\n","# 정답 = [음 음 음 음 정 정]\n","# 예측 = [음 음 정 정 정 정]\n","\n","# ex) 정확도 : 나의 예측이 얼마나 정확한가? 4/6 => 66%\n","# ex) 정밀도 : 나의 참이라고 예측한 것이 얼마나 맞았나? 2/4 => 50%\n","# ex) 재현율 : 전체 참 중에서 내가 참이라 예측한 것은? 2/2 => 100%"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HrwUJSTphuhH","colab_type":"code","outputId":"0ed7414f-50ce-44b8-d1f6-91c299c3a168","executionInfo":{"status":"ok","timestamp":1561959449123,"user_tz":-540,"elapsed":839,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["print('정확도', accuracy_score(y_test, mypred))\n","print('정밀도', precision_score(y_test, mypred))\n","print('재현율(민감도)', recall_score(y_test, mypred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["정확도 0.9\n","정밀도 0.0\n","재현율(민감도) 0.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"yQ7-FjZwmCf1","colab_type":"code","colab":{}},"source":["# MyDummyClassifier의 결과를 혼동행렬로 분석해봄\n","\n","# 타이타닉 데이터셋을 이용해서 의사결정나무 와 로지스틱회귀 알고리즘으로\n","# 분류한 후 정확도/정밀도/재현율을 확인해 봄\n","\n","# 정밀도와 재현율 관계를 알아볼 것"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaeMlQeY7F1u","colab_type":"code","outputId":"40c1f0d9-f16a-4aaf-fca5-ee2f9da23b94","executionInfo":{"status":"ok","timestamp":1561959451068,"user_tz":-540,"elapsed":833,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["X_train, X_test, y_train, y_test = train_test_split(titanic_data, titanic_target, test_size=0.2, random_state=0)\n","\n","my_clf = MyDummyClassifier()\n","my_clf.fit(X_train, y_train)\n","\n","mypred = my_clf.predict(X_test)\n","\n","print('정확도', accuracy_score(y_test, mypred))\n","print('정밀도', precision_score(y_test, mypred))\n","print('재현율(민감도)', recall_score(y_test, mypred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["정확도 0.7877094972067039\n","정밀도 0.7313432835820896\n","재현율(민감도) 0.7101449275362319\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cSNhu0nl8c2b","colab_type":"code","outputId":"70a7838a-4894-4c10-cbc0-93d2e3d82b24","executionInfo":{"status":"ok","timestamp":1561965780152,"user_tz":-540,"elapsed":907,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":371}},"source":["# 타이타닉 데이터셋을 이용해서\n","# 의사결정나무, 로지스틱회귀\n","# 분류한 후 정확도/정밀도/재현율을 확인해 봄\n","\n","dt_clf = DecisionTreeClassifier()\n","dt_clf.fit(X_train, y_train)\n","pred = dt_clf.predict(X_test)\n","\n","print('정확도', accuracy_score(y_test, pred))\n","print('정밀도', precision_score(y_test, pred))\n","print('재현율(민감도)', recall_score(y_test, pred))\n","\n","\n","print(pred)\n","\n","print('---------------------------------------------------')\n","\n","lg_clf = LogisticRegression()\n","lg_clf.fit(X_train, y_train)\n","pred = dt_clf.predict(X_test)\n","\n","print('정확도', accuracy_score(y_test, pred))\n","print('정밀도', precision_score(y_test, pred))\n","print('재현율(민감도)', recall_score(y_test, pred))\n","\n","print(pred)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["정확도 0.7932960893854749\n","정밀도 0.7580645161290323\n","재현율(민감도) 0.6811594202898551\n","[0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0\n"," 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0\n"," 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n"," 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n"," 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1]\n","---------------------------------------------------\n","정확도 0.7932960893854749\n","정밀도 0.7580645161290323\n","재현율(민감도) 0.6811594202898551\n","[0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0\n"," 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0\n"," 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n"," 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n"," 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"A_so1z3o96Fk","colab_type":"code","colab":{}},"source":["# 정밀도/재현율 trade-off\n","# 분류하는 업무의 특성상 정밀도 또는 재현율이 특별히 강조되어야 하는 경우 (특히 임상의료)\n","# 결정 임계값을 조정하면 정밀도 또는 재현율을 높일 수 있음\n","\n","# 즉, 이진분류에서 0 또는 1로 판정할 기준값의 범위를 의미함.\n","# 임계값을 0.5로 정하는 경우 기준값보다 확률이 크면 positive, 작으면 negative로 결정\n","\n","# 한편, 정밀도와 재현율은 상호보완적 지표이기 때문에, 어느 한쪽을 올리면 다른 한쪽은 떨어지는 관계\n","\n","# F1 스코어\n","# 정밀도, 재현율을 결합한 지표\n","# 어느 한쪽으로 치우치지 않은 수치를 나타낼 때 상대적으로 높은 값을 가짐\n","\n","# F1 = 2 / ((1/재현율) + (1/정밀도))\n","\n","# 예를 들어, 예측모델 A의 정밀도와 재현율이\n","# 0.9/0.1이고, 예측모델 B의 정밀도와 재현율이\n","# 0.55/0.45일때 각각의 F1스코어는 0.18/0.495이다\n","# 따라서, 모델 A는 모델 B보다 덜 우수하다"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJSSDXtmAZHL","colab_type":"code","outputId":"4eff828d-d39c-4e19-df93-e1e72dacca9c","executionInfo":{"status":"ok","timestamp":1561965752775,"user_tz":-540,"elapsed":679,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from  sklearn.metrics import f1_score\n","\n","my_clf=MyDummyClassifier()\n","\n","my_clf.fit(X_train, y_train)\n","\n","mypred = my_clf.predict(X_test)\n","\n","# print(mypred)\n","print('F1 스코어', f1_score(y_test, mypred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["F1 스코어 0.7205882352941175\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kr-H4z6SCu_T","colab_type":"code","colab":{}},"source":["# 타이타닉 데이터셋을 이용해서 의사결정나무와 로지스틱회귀 알고리즘으로 분류한 후 F1 score 확인"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABcpa0moC112","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":484},"outputId":"c1c000f3-1663-48e6-fd27-e0794328bf75","executionInfo":{"status":"ok","timestamp":1562037740851,"user_tz":-540,"elapsed":768,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}}},"source":["# ROC\n","\n","# Receiver Operation Characteristics Curve\n","# 수신자 판단 곡선\n","\n","# 세계 2차 대전 통신 장비 성능 평가를 위해 고안된 수치\n","# 의학분야에 많이 사용되지만, 머신러닝의 이진 분류 모델 예측 성능 평가에도 사용\n","\n","# 특이도(FPR)가 변할 때 민감도가 어떻게 변하는지 알아보기 위한 곡선\n","\n","# 환자 중 보균자p/정상인n 있는 경우\n","# 재현율 : 보균자를 보균자로 양성 판정\n","# 특이도 : 정상인을 정상인으로 음성 판정\n","\n","# -_-;;; -------------------------------------------------------------------------------\n","\n","\n","# AUC \n","\n","# Area Under Curve\n","# ROC 곡선 밑의 면적을 구한 값\n","# 1에 가까울수록 좋은 수치를 의미함\n","\n","import pandas as pd\n","\n","titanic = pd.read_csv('titanic.csv')\n","titanic.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n","0            1         0       3  ...   7.2500   NaN         S\n","1            2         1       1  ...  71.2833   C85         C\n","2            3         1       3  ...   7.9250   NaN         S\n","3            4         1       1  ...  53.1000  C123         S\n","4            5         0       3  ...   8.0500   NaN         S\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"_HGvYutpRXxs","colab_type":"code","outputId":"8944ef87-48bb-469f-8761-9010b3a094ae","executionInfo":{"status":"ok","timestamp":1561972197140,"user_tz":-540,"elapsed":989,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.metrics import roc_curve\n","\n","# 타이타닉 데이터셋을 로지스틱 회귀 분류로 분석한 결과의\n","# FPR, TPR 을 구하고 ROC 그래프를 작성\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","titanic = pd.read_csv('titanic.csv')\n","titanic_target = titanic['Survived']\n","titanic_data = titanic.drop('Survived', axis=1)\n","\n","\n","# 데이터 전처리\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","# null 처리\n","titanic['Age'].fillna(titanic['Age'].mean(), inplace=True)\n","titanic['Cabin'].fillna('N', inplace=True)\n","titanic['Embarked'].fillna('N', inplace=True)\n","titanic['Fare'].fillna(0, inplace=True)\n","\n","\n","# 불필요한 속성 제거\n","titanic.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n","\n","\n","# 레이블 인코딩 적용\n","titanic['Cabin'] = titanic['Cabin'].str[:1]\n","# 선실 위치의 첫글자를 추출(deck 단위)\n","\n","features = ['Cabin', 'Sex', 'Embarked']\n","for feature in features:\n","  le = LabelEncoder()\n","  le = le.fit(titanic[feature])\n","  titanic[feature] = le.transform(titanic[feature])\n","\n","\n","titanic_target = titanic['Survived']\n","titanic_data = titanic.drop('Survived', axis=1)\n","\n","print(titanic)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["     Survived  Pclass  Sex        Age  SibSp  Parch      Fare  Cabin  Embarked\n","0           0       3    1  22.000000      1      0    7.2500      7         3\n","1           1       1    0  38.000000      1      0   71.2833      2         0\n","2           1       3    0  26.000000      0      0    7.9250      7         3\n","3           1       1    0  35.000000      1      0   53.1000      2         3\n","4           0       3    1  35.000000      0      0    8.0500      7         3\n","5           0       3    1  29.699118      0      0    8.4583      7         2\n","6           0       1    1  54.000000      0      0   51.8625      4         3\n","7           0       3    1   2.000000      3      1   21.0750      7         3\n","8           1       3    0  27.000000      0      2   11.1333      7         3\n","9           1       2    0  14.000000      1      0   30.0708      7         0\n","10          1       3    0   4.000000      1      1   16.7000      6         3\n","11          1       1    0  58.000000      0      0   26.5500      2         3\n","12          0       3    1  20.000000      0      0    8.0500      7         3\n","13          0       3    1  39.000000      1      5   31.2750      7         3\n","14          0       3    0  14.000000      0      0    7.8542      7         3\n","15          1       2    0  55.000000      0      0   16.0000      7         3\n","16          0       3    1   2.000000      4      1   29.1250      7         2\n","17          1       2    1  29.699118      0      0   13.0000      7         3\n","18          0       3    0  31.000000      1      0   18.0000      7         3\n","19          1       3    0  29.699118      0      0    7.2250      7         0\n","20          0       2    1  35.000000      0      0   26.0000      7         3\n","21          1       2    1  34.000000      0      0   13.0000      3         3\n","22          1       3    0  15.000000      0      0    8.0292      7         2\n","23          1       1    1  28.000000      0      0   35.5000      0         3\n","24          0       3    0   8.000000      3      1   21.0750      7         3\n","25          1       3    0  38.000000      1      5   31.3875      7         3\n","26          0       3    1  29.699118      0      0    7.2250      7         0\n","27          0       1    1  19.000000      3      2  263.0000      2         3\n","28          1       3    0  29.699118      0      0    7.8792      7         2\n","29          0       3    1  29.699118      0      0    7.8958      7         3\n","..        ...     ...  ...        ...    ...    ...       ...    ...       ...\n","861         0       2    1  21.000000      1      0   11.5000      7         3\n","862         1       1    0  48.000000      0      0   25.9292      3         3\n","863         0       3    0  29.699118      8      2   69.5500      7         3\n","864         0       2    1  24.000000      0      0   13.0000      7         3\n","865         1       2    0  42.000000      0      0   13.0000      7         3\n","866         1       2    0  27.000000      1      0   13.8583      7         0\n","867         0       1    1  31.000000      0      0   50.4958      0         3\n","868         0       3    1  29.699118      0      0    9.5000      7         3\n","869         1       3    1   4.000000      1      1   11.1333      7         3\n","870         0       3    1  26.000000      0      0    7.8958      7         3\n","871         1       1    0  47.000000      1      1   52.5542      3         3\n","872         0       1    1  33.000000      0      0    5.0000      1         3\n","873         0       3    1  47.000000      0      0    9.0000      7         3\n","874         1       2    0  28.000000      1      0   24.0000      7         0\n","875         1       3    0  15.000000      0      0    7.2250      7         0\n","876         0       3    1  20.000000      0      0    9.8458      7         3\n","877         0       3    1  19.000000      0      0    7.8958      7         3\n","878         0       3    1  29.699118      0      0    7.8958      7         3\n","879         1       1    0  56.000000      0      1   83.1583      2         0\n","880         1       2    0  25.000000      0      1   26.0000      7         3\n","881         0       3    1  33.000000      0      0    7.8958      7         3\n","882         0       3    0  22.000000      0      0   10.5167      7         3\n","883         0       2    1  28.000000      0      0   10.5000      7         3\n","884         0       3    1  25.000000      0      0    7.0500      7         3\n","885         0       3    0  39.000000      0      5   29.1250      7         2\n","886         0       2    1  27.000000      0      0   13.0000      7         3\n","887         1       1    0  19.000000      0      0   30.0000      1         3\n","888         0       3    0  29.699118      1      2   23.4500      7         3\n","889         1       1    1  26.000000      0      0   30.0000      2         0\n","890         0       3    1  32.000000      0      0    7.7500      7         2\n","\n","[891 rows x 9 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BIHcFTiTyfwK","colab_type":"code","outputId":"1527ad5c-1fb2-478d-e31f-57660f5675e2","executionInfo":{"status":"ok","timestamp":1561972421033,"user_tz":-540,"elapsed":882,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 분류 알고리즘 종류\n","\n","# 판별함수 모형 : 주어진 데이터를 범주category에 따라 서로 다른 영역으로\n","#                 나누는 경계면을 찾아낸 후, 이 경계면(decision boundary)을 \n","#                 기준으로 데이터가 어디에 있는지를 계산하는 함수\n","\n","# 확률적 판별/생성 모형 : 주어진 데이터에 대해 각 범주 category / 레이블이 \n","#                    정답일 조건부확률을 계산하는 방법에 따라 \n","#                    조건부 확률 함수를 추정 하거나 베이즈 정리를 사용해서 \n","#                    분류하는 모형\n","\n","# 따라서, 분류기의 예측 불확실성을 추정하려면 sci-kit learn 에서 \n","# 제공하는 2가지 함수를 사용\n","# => decision function, predict_proba\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","X_train, X_test, y_train, y_test = \\\n","train_test_split(titanic_data, titanic_target, test_size=0.2, random_state=0)\n","\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","lg_clf = LogisticRegression()\n","\n","\n","lg_clf.fit(X_train, y_train)\n","pred = lg_clf.predict(X_test)\n","\n","# 타이타닉 생존예측확률 알아보기 \n","\n","print(lg_clf.predict(X_test))         # 이산형 수치\n","print(lg_clf.predict(X_test).shape)  \n","print(lg_clf.predict_proba(X_test))  \n","print(lg_clf.predict_proba(X_test).shape)  \n","\n","\n","print('-----------------------------------------------')\n","\n","print(lg_clf.predict(X_test[:5]))         # 이산형 수치 => 걍 예측값을 0 또는 1 둘 중 1개로 뱉어냄\n","print(lg_clf.predict_proba(X_test[:5]))   # 확률 수치 => 예측값을 2개로 뱉어냄 (0~1 사이 실수 2개로. 앞열은 0, 뒷열은 1에 대한 확률)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0\n"," 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0\n"," 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0\n"," 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1\n"," 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0]\n","(179,)\n","[[0.85157347 0.14842653]\n"," [0.87164664 0.12835336]\n"," [0.9239038  0.0760962 ]\n"," [0.08235587 0.91764413]\n"," [0.35114951 0.64885049]\n"," [0.58232183 0.41767817]\n"," [0.08988965 0.91011035]\n"," [0.07387902 0.92612098]\n"," [0.53027821 0.46972179]\n"," [0.340053   0.659947  ]\n"," [0.89473093 0.10526907]\n"," [0.2789668  0.7210332 ]\n"," [0.86625082 0.13374918]\n"," [0.13132471 0.86867529]\n"," [0.0666739  0.9333261 ]\n"," [0.26974108 0.73025892]\n"," [0.84658417 0.15341583]\n"," [0.759666   0.240334  ]\n"," [0.88816864 0.11183136]\n"," [0.72057093 0.27942907]\n"," [0.69815538 0.30184462]\n"," [0.05845281 0.94154719]\n"," [0.86625689 0.13374311]\n"," [0.59788564 0.40211436]\n"," [0.31760744 0.68239256]\n"," [0.12342427 0.87657573]\n"," [0.87900259 0.12099741]\n"," [0.31950638 0.68049362]\n"," [0.19761617 0.80238383]\n"," [0.41913481 0.58086519]\n"," [0.86646826 0.13353174]\n"," [0.34529342 0.65470658]\n"," [0.86489629 0.13510371]\n"," [0.57721705 0.42278295]\n"," [0.90319047 0.09680953]\n"," [0.56174368 0.43825632]\n"," [0.91038405 0.08961595]\n"," [0.75805459 0.24194541]\n"," [0.73919137 0.26080863]\n"," [0.87052598 0.12947402]\n"," [0.78082272 0.21917728]\n"," [0.83707878 0.16292122]\n"," [0.86835285 0.13164715]\n"," [0.93752192 0.06247808]\n"," [0.13764892 0.86235108]\n"," [0.87148443 0.12851557]\n"," [0.87148443 0.12851557]\n"," [0.05697719 0.94302281]\n"," [0.81165848 0.18834152]\n"," [0.77005864 0.22994136]\n"," [0.58605889 0.41394111]\n"," [0.47989323 0.52010677]\n"," [0.15698113 0.84301887]\n"," [0.83116385 0.16883615]\n"," [0.54884685 0.45115315]\n"," [0.80707574 0.19292426]\n"," [0.78846637 0.21153363]\n"," [0.5659452  0.4340548 ]\n"," [0.91321045 0.08678955]\n"," [0.92244669 0.07755331]\n"," [0.8244564  0.1755436 ]\n"," [0.40247143 0.59752857]\n"," [0.20249407 0.79750593]\n"," [0.55501945 0.44498055]\n"," [0.37356113 0.62643887]\n"," [0.8740309  0.1259691 ]\n"," [0.1827432  0.8172568 ]\n"," [0.75017585 0.24982415]\n"," [0.14763146 0.85236854]\n"," [0.08021134 0.91978866]\n"," [0.2411139  0.7588861 ]\n"," [0.7551755  0.2448245 ]\n"," [0.58130298 0.41869702]\n"," [0.87141204 0.12858796]\n"," [0.85294394 0.14705606]\n"," [0.43211512 0.56788488]\n"," [0.53757077 0.46242923]\n"," [0.6408562  0.3591438 ]\n"," [0.88582127 0.11417873]\n"," [0.73772607 0.26227393]\n"," [0.89434269 0.10565731]\n"," [0.83878713 0.16121287]\n"," [0.26213066 0.73786934]\n"," [0.81826065 0.18173935]\n"," [0.83422509 0.16577491]\n"," [0.0721578  0.9278422 ]\n"," [0.06668986 0.93331014]\n"," [0.61384055 0.38615945]\n"," [0.24135069 0.75864931]\n"," [0.46265607 0.53734393]\n"," [0.60988617 0.39011383]\n"," [0.83116385 0.16883615]\n"," [0.66605621 0.33394379]\n"," [0.13053055 0.86946945]\n"," [0.47083324 0.52916676]\n"," [0.85536184 0.14463816]\n"," [0.20895963 0.79104037]\n"," [0.94093797 0.05906203]\n"," [0.789562   0.210438  ]\n"," [0.4828652  0.5171348 ]\n"," [0.95548524 0.04451476]\n"," [0.8995049  0.1004951 ]\n"," [0.82754823 0.17245177]\n"," [0.87570141 0.12429859]\n"," [0.45438406 0.54561594]\n"," [0.49222464 0.50777536]\n"," [0.21293668 0.78706332]\n"," [0.48445699 0.51554301]\n"," [0.78979623 0.21020377]\n"," [0.43334627 0.56665373]\n"," [0.92144257 0.07855743]\n"," [0.08605502 0.91394498]\n"," [0.85561774 0.14438226]\n"," [0.37125287 0.62874713]\n"," [0.63935446 0.36064554]\n"," [0.21269098 0.78730902]\n"," [0.40282323 0.59717677]\n"," [0.0436424  0.9563576 ]\n"," [0.90224992 0.09775008]\n"," [0.38526574 0.61473426]\n"," [0.84689964 0.15310036]\n"," [0.86590656 0.13409344]\n"," [0.8348803  0.1651197 ]\n"," [0.68460886 0.31539114]\n"," [0.88181937 0.11818063]\n"," [0.73688393 0.26311607]\n"," [0.85536402 0.14463598]\n"," [0.89191019 0.10808981]\n"," [0.77502436 0.22497564]\n"," [0.8608061  0.1391939 ]\n"," [0.30311161 0.69688839]\n"," [0.83931184 0.16068816]\n"," [0.85533057 0.14466943]\n"," [0.38031845 0.61968155]\n"," [0.80912296 0.19087704]\n"," [0.83931184 0.16068816]\n"," [0.90066297 0.09933703]\n"," [0.52795003 0.47204997]\n"," [0.86903031 0.13096969]\n"," [0.72748609 0.27251391]\n"," [0.75612796 0.24387204]\n"," [0.12252702 0.87747298]\n"," [0.87141204 0.12858796]\n"," [0.28533893 0.71466107]\n"," [0.20722531 0.79277469]\n"," [0.33163796 0.66836204]\n"," [0.76699701 0.23300299]\n"," [0.3525612  0.6474388 ]\n"," [0.07509339 0.92490661]\n"," [0.87112416 0.12887584]\n"," [0.71191744 0.28808256]\n"," [0.36902946 0.63097054]\n"," [0.42195425 0.57804575]\n"," [0.86918    0.13082   ]\n"," [0.1451079  0.8548921 ]\n"," [0.74690604 0.25309396]\n"," [0.45424105 0.54575895]\n"," [0.89657572 0.10342428]\n"," [0.36074336 0.63925664]\n"," [0.29830582 0.70169418]\n"," [0.87148443 0.12851557]\n"," [0.81562337 0.18437663]\n"," [0.22468667 0.77531333]\n"," [0.63378395 0.36621605]\n"," [0.84883703 0.15116297]\n"," [0.85169327 0.14830673]\n"," [0.92792609 0.07207391]\n"," [0.84344857 0.15655143]\n"," [0.85113868 0.14886132]\n"," [0.82212636 0.17787364]\n"," [0.92764651 0.07235349]\n"," [0.13779555 0.86220445]\n"," [0.87178722 0.12821278]\n"," [0.86835285 0.13164715]\n"," [0.23814041 0.76185959]\n"," [0.87141204 0.12858796]\n"," [0.09921962 0.90078038]\n"," [0.84732876 0.15267124]\n"," [0.83107315 0.16892685]]\n","(179, 2)\n","-----------------------------------------------\n","[0 0 0 1 1]\n","[[0.85157347 0.14842653]\n"," [0.87164664 0.12835336]\n"," [0.9239038  0.0760962 ]\n"," [0.08235587 0.91764413]\n"," [0.35114951 0.64885049]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MnncUhJZzjRD","colab_type":"code","outputId":"72bc9314-403e-40a1-ce7c-eb1c1a841ad7","executionInfo":{"status":"ok","timestamp":1561972862383,"user_tz":-540,"elapsed":868,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# 특이도별 재현율의 변화를 살펴보기 위해 \n","# 샘플로 10건을 추출해서 비교해 봄\n","\n","pred_prop = lg_clf.predict_proba(X_test)[:, 1]\n","# 레이블이 1일때 각 예측확률 구함\n","\n","print('예측확률 건수', lg_clf.predict_proba(X_test).shape)\n","print('-----------------------')\n","print('예측확률 건수', pred_prop.shape) # 위에서 뒷열 1개 열만 전체 행으로 가져옴\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["예측확률 건수 (179, 2)\n","-----------------------\n","예측확률 건수 (179,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rklhq-Zv1bPD","colab_type":"code","outputId":"aac14f32-5445-44ba-b3ae-6d0481087d82","executionInfo":{"status":"ok","timestamp":1561973155059,"user_tz":-540,"elapsed":1040,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":738}},"source":["fprs, tprs, thresholds = roc_curve(y_test, pred_prop)\n","# 각 예측확률별 특이도와 민감도, 임계값을 구함\n","\n","\n","thr_idx = np.arange(0, thresholds.shape[0], 5)\n","# 임계값 간격을 0 ~ 67에서 5씩 나눠 생성\n","\n","print('----------------------------------------------------------------- 특이도↓')\n","print(fprs)\n","print('----------------------------------------------------------------- 민감도↓')\n","print(tprs)\n","print('----------------------------------------------------------------- 임계값↓')\n","print(thresholds)\n","print('----------------------------------------------------------------- 임계값 5씩 나눈 idx↓')\n","print(thr_idx)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------- 특이도↓\n","[0.         0.         0.         0.01818182 0.01818182 0.02727273\n"," 0.02727273 0.04545455 0.04545455 0.05454545 0.05454545 0.06363636\n"," 0.06363636 0.07272727 0.07272727 0.08181818 0.08181818 0.09090909\n"," 0.09090909 0.10909091 0.10909091 0.14545455 0.14545455 0.15454545\n"," 0.15454545 0.16363636 0.16363636 0.17272727 0.17272727 0.18181818\n"," 0.18181818 0.19090909 0.19090909 0.2        0.2        0.21818182\n"," 0.21818182 0.22727273 0.22727273 0.25454545 0.25454545 0.30909091\n"," 0.30909091 0.36363636 0.36363636 0.41818182 0.41818182 0.46363636\n"," 0.48181818 0.50909091 0.50909091 0.52727273 0.54545455 0.54545455\n"," 0.62727273 0.62727273 0.68181818 0.7        0.73636364 0.79090909\n"," 0.86363636 0.86363636 0.88181818 0.88181818 0.94545455 0.94545455\n"," 1.        ]\n","----------------------------------------------------------------- 민감도↓\n","[0.         0.01449275 0.47826087 0.47826087 0.49275362 0.49275362\n"," 0.50724638 0.50724638 0.52173913 0.52173913 0.57971014 0.57971014\n"," 0.5942029  0.5942029  0.60869565 0.60869565 0.62318841 0.62318841\n"," 0.63768116 0.63768116 0.66666667 0.66666667 0.69565217 0.69565217\n"," 0.71014493 0.71014493 0.72463768 0.72463768 0.73913043 0.73913043\n"," 0.75362319 0.75362319 0.76811594 0.76811594 0.79710145 0.79710145\n"," 0.84057971 0.84057971 0.85507246 0.85507246 0.86956522 0.86956522\n"," 0.88405797 0.88405797 0.89855072 0.89855072 0.91304348 0.91304348\n"," 0.91304348 0.91304348 0.92753623 0.92753623 0.92753623 0.94202899\n"," 0.94202899 0.95652174 0.95652174 0.95652174 0.95652174 0.95652174\n"," 0.95652174 0.97101449 0.97101449 0.98550725 0.98550725 1.\n"," 1.        ]\n","----------------------------------------------------------------- 임계값↓\n","[1.9563576  0.9563576  0.75864931 0.73025892 0.7210332  0.71466107\n"," 0.70169418 0.68239256 0.68049362 0.66836204 0.6474388  0.63925664\n"," 0.63097054 0.62874713 0.62643887 0.61968155 0.61473426 0.59752857\n"," 0.59717677 0.57804575 0.56665373 0.52916676 0.5171348  0.51554301\n"," 0.50777536 0.47204997 0.46972179 0.46242923 0.45115315 0.44498055\n"," 0.43825632 0.4340548  0.42278295 0.41869702 0.41394111 0.39011383\n"," 0.36064554 0.3591438  0.33394379 0.28808256 0.27942907 0.24982415\n"," 0.2448245  0.22497564 0.21917728 0.18834152 0.18437663 0.16892685\n"," 0.16883615 0.16292122 0.16121287 0.16068816 0.15341583 0.15310036\n"," 0.14463598 0.14438226 0.13353174 0.13164715 0.12887584 0.12851557\n"," 0.11183136 0.10808981 0.10526907 0.10342428 0.07855743 0.07755331\n"," 0.04451476]\n","----------------------------------------------------------------- 임계값 5씩 나눈 idx↓\n","[ 0  5 10 15 20 25 30 35 40 45 50 55 60 65]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UsMLMWYm2P9r","colab_type":"code","outputId":"53250547-f60b-4472-b62a-a2bb4a604cd9","executionInfo":{"status":"ok","timestamp":1561973504703,"user_tz":-540,"elapsed":914,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["print('임계값 간격', thr_idx)\n","print('샘플용 임계값', np.round(thresholds[thr_idx], 2)) # thr_idx 임의로 14개\n","# 총 14개의 임계값 인덱스 출력\n","# [1.96 0.71 0.65 0.62 0.57 0.47 0.44 \n","#  0.39 0.28 0.19 0.16 0.14 0.11 0.08]\n","\n","print('샘플용 임계값별 FPR', np.round(fprs[thr_idx], 3)) # thr_idx 임의로 14개\n","print('샘플용 임계값별 TPR', np.round(tprs[thr_idx], 3)) # thr_idx 임의로 14개\n","# 총 14개의 특이도와 민감도를 출력\n","\n","\n","# ROC 그래프 그림\n","plt.plot(fprs, tprs, label='ROC') # label  뭐꼬?\n","# plt.plot([0,10], [0,10], 'k--', label='Random') # 앞에 bracket묶음 2개는 각 x값, y값 범위 <- 이 범위로 하면 잘 안 보임\n","plt.plot([0,1], [0,1], 'k--', label='Random') # 앞에 bracket묶음 2개는 각 x값, y값 범위\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["임계값 간격 [ 0  5 10 15 20 25 30 35 40 45 50 55 60 65]\n","샘플용 임계값 [1.96 0.71 0.65 0.62 0.57 0.47 0.44 0.39 0.28 0.19 0.16 0.14 0.11 0.08]\n","샘플용 임계값별 FPR [0.    0.027 0.055 0.082 0.109 0.164 0.182 0.218 0.255 0.418 0.509 0.627\n"," 0.864 0.945]\n","샘플용 임계값별 TPR [0.    0.493 0.58  0.609 0.667 0.71  0.754 0.797 0.87  0.899 0.928 0.957\n"," 0.957 1.   ]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"7ZGUzsey3fk7","colab_type":"code","outputId":"e8fabc5c-6be2-4a7b-f5a6-3a838d271c5e","executionInfo":{"status":"ok","timestamp":1561973524887,"user_tz":-540,"elapsed":889,"user":{"displayName":"얍얍","photoUrl":"","userId":"00095991694523741423"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["## AUC (Area Under Curve)\n","\n","# ROC 곡선 밑의 면적을 구한 값\n","# 1에 가까울 수록 좋은 수치를 의미함\n","# 0.9 ~   1 : excellent\n","# 0.8 ~ 0.9 : good\n","# 0.7 ~ 0.8 : normal\n","\n","\n","from sklearn.metrics import roc_auc_score\n","\n","\n","pred = lg_clf.predict(X_test)\n","roc_score = roc_auc_score(y_test, pred)\n","print('ROC AUC 점수', roc_score)\n","\n","print(lg_clf.predict(X_test[:5])) # 이산형 수치\n","print('---------------------------------')\n","print(lg_clf.predict_proba(X_test[:5])) # 확률 수치"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ROC AUC 점수 0.7777997364953887\n","[0 0 0 1 1]\n","---------------------------------\n","[[0.85157347 0.14842653]\n"," [0.87164664 0.12835336]\n"," [0.9239038  0.0760962 ]\n"," [0.08235587 0.91764413]\n"," [0.35114951 0.64885049]]\n"],"name":"stdout"}]}]}